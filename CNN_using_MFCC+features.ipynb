{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9oYtCNliXLP"
      },
      "outputs": [],
      "source": [
        "import parselmouth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "D4VUHn4JmgTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/MyDrive/audio_speech_actors_01-24'"
      ],
      "metadata": {
        "id": "03oZTzj2nGfS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import torch\n",
        "import parselmouth\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/audio_speech_actors_01-24'\n",
        "\n",
        "paths = []\n",
        "labels = []\n",
        "for dirname, _, filenames in os.walk(base_dir):\n",
        "    for filename in filenames:\n",
        "        parts = filename.replace(\".wav\", \"\").split(\"-\")\n",
        "        paths.append(os.path.join(dirname, filename))\n",
        "        labels.append(int(parts[2]))\n",
        "\n",
        "df = pd.DataFrame({\"speech\": paths, \"labels\": labels})\n",
        "le = LabelEncoder()\n",
        "df[\"encoded_labels\"] = le.fit_transform(df[\"labels\"])\n",
        "\n",
        "def extract_mfcc(path, n_mfcc=40, max_len=360):\n",
        "    signal, sr = librosa.load(path, sr=None)\n",
        "    mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=n_mfcc)\n",
        "    if mfcc.shape[1] < max_len:\n",
        "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, max_len - mfcc.shape[1])), mode='constant')\n",
        "    else:\n",
        "        mfcc = mfcc[:, :max_len]\n",
        "    mfcc = (mfcc - np.mean(mfcc)) / np.std(mfcc)\n",
        "    return mfcc\n",
        "\n",
        "def extract_acoustic_features(path):\n",
        "    snd = parselmouth.Sound(path)\n",
        "    pitch = snd.to_pitch()\n",
        "    point_process = parselmouth.praat.call(snd, \"To PointProcess (periodic, cc)\", 75, 500)\n",
        "\n",
        "    jitter = parselmouth.praat.call([point_process], \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
        "    shimmer = parselmouth.praat.call([snd, point_process], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
        "\n",
        "    duration = snd.get_total_duration()\n",
        "    num_pulses = parselmouth.praat.call(point_process, \"Get number of points\")\n",
        "    speaking_rate = num_pulses / duration if duration > 0 else 0\n",
        "\n",
        "    return np.array([jitter, shimmer, speaking_rate], dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "mfcc_features = []\n",
        "acoustic_features = []\n",
        "targets = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    mfcc = extract_mfcc(df.iloc[i][\"speech\"])\n",
        "    acoustic = extract_acoustic_features(df.iloc[i][\"speech\"])\n",
        "    label = torch.tensor(df.iloc[i][\"encoded_labels\"], dtype=torch.long)\n",
        "\n",
        "    mfcc_features.append(torch.tensor(mfcc, dtype=torch.float32).unsqueeze(0))\n",
        "    acoustic_features.append(torch.tensor(acoustic))\n",
        "    targets.append(label)\n",
        "\n",
        "mfcc_tensor = torch.stack(mfcc_features)\n",
        "acoustic_tensor = torch.stack(acoustic_features)\n",
        "target_tensor = torch.stack(targets)\n",
        "\n",
        "# NORMALIZE ACOUSTIC FEATURES\n",
        "scaler = StandardScaler()\n",
        "acoustic_tensor = torch.tensor(scaler.fit_transform(acoustic_tensor.numpy()), dtype=torch.float32)\n",
        "\n",
        "# CNN FEATURE EXTRACTOR\n",
        "cnn_model = nn.Sequential(\n",
        "    nn.Conv2d(1, 16, kernel_size=3),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "    nn.Conv2d(16, 32, kernel_size=3),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "    nn.Flatten()\n",
        ")\n",
        "\n",
        "# EXTRACT CNN OUTPUT FEATURES\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cnn_model.to(device)\n",
        "cnn_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    cnn_output = cnn_model(mfcc_tensor.to(device)).cpu()\n",
        "\n",
        "# CONCATENATE WITH ACOUSTIC FEATURES\n",
        "combined_features = torch.cat([cnn_output, acoustic_tensor], dim=1)\n",
        "\n",
        "# FINAL FFNN CLASSIFIER\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(22531, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, len(le.classes_))\n",
        ").to(device)\n",
        "\n",
        "#CREATE DATASET AND LOADERS\n",
        "dataset = TensorDataset(combined_features, target_tensor)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32)\n",
        "\n",
        "#TRAINING SETUP\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "#TRAINING LOOP\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "#VALIDATION\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        pred = torch.argmax(model(x), dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axKjw_IRopfU",
        "outputId": "ba0439a2-560b-42a9-e943-d16524fb5931"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.0522\n",
            "Epoch 2, Loss: 1.9033\n",
            "Epoch 3, Loss: 1.7121\n",
            "Epoch 4, Loss: 1.5918\n",
            "Epoch 5, Loss: 1.5408\n",
            "Epoch 6, Loss: 1.4909\n",
            "Epoch 7, Loss: 1.4138\n",
            "Epoch 8, Loss: 1.4037\n",
            "Epoch 9, Loss: 1.3607\n",
            "Epoch 10, Loss: 1.2797\n",
            "Validation Accuracy: 44.44%\n"
          ]
        }
      ]
    }
  ]
}